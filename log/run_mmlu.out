nohup: ignoring input
==========================================
MMLU æ•°æ®é›† DPO æ•°æ®ç”Ÿæˆ (æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹ä¿å­˜)
==========================================
GPU: 6,7
æ‰¹æ¬¡å¤§å°: 128
å¹¶å‘æ•°: 30
è¾“å‡ºç›®å½•: output/mmlu
==========================================

MMLU å…±æœ‰ 10 ä¸ªä»»åŠ¡ (æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹ä¿å­˜)

----------------------------------------
å¤„ç†ä»»åŠ¡ [1/10]: college_computer_science
----------------------------------------
å¼€å§‹ç”Ÿæˆ DPO æ•°æ®...
è¾“å‡ºæ–‡ä»¶: output/mmlu/dpo_college_computer_science.jsonl
2025-12-29 18:45:53,521 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,521 - __main__ - INFO - æ•°æ®é›†åç§°: mmlu
2025-12-29 18:45:53,521 - __main__ - INFO - æ•°æ®é›†è·¯å¾„: dataset/mmlu/college_computer_science.json
2025-12-29 18:45:53,521 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,521 - __main__ - INFO - ä½¿ç”¨æ•°æ®é›†é€‚é…å±‚åŠ è½½: mmlu
2025-12-29 18:45:53,521 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,521 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] å¼€å§‹åŠ è½½æ•°æ®é›†: mmlu
2025-12-29 18:45:53,521 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] æ–‡ä»¶è·¯å¾„: dataset/mmlu/college_computer_science.json
2025-12-29 18:45:53,521 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,522 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] å·²åŠ è½½ JSON æ–‡ä»¶
2025-12-29 18:45:53,522 - __main__ - INFO - é¢„å¤„ç† MMLU æ•°æ®é›†: 100 æ¡
2025-12-29 18:45:53,522 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] é¢„å¤„ç†å®Œæˆ: 100 æ¡æœ‰æ•ˆæ•°æ®
2025-12-29 18:45:53,522 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] æ•°æ®æ ¼å¼å·²ç»Ÿä¸€ä¸º: {'question': str, 'answer': str}
2025-12-29 18:45:53,522 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,522 - __main__ - INFO - æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± 100 æ¡æ•°æ®
2025-12-29 18:45:53,522 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,522 - __main__ - INFO - Step 1: å¼€å§‹ç”ŸæˆDPOæ•°æ®
2025-12-29 18:45:53,522 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,522 - __main__ - INFO - æœªå‘ç°å·²æœ‰è¿›åº¦ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†
2025-12-29 18:45:53,522 - __main__ - INFO - å…±éœ€å¤„ç† 100 æ¡æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: 64
2025-12-29 18:45:53,522 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,522 - __main__ - INFO - é˜¶æ®µ1/3: vLLMåˆ†æ‰¹æœ¬åœ°æ¨ç†ï¼ˆæ‰€æœ‰æ•°æ®ï¼‰
2025-12-29 18:45:53,523 - __main__ - INFO - ============================================================
2025-12-29 18:45:53,523 - __main__ - INFO - å¤„ç†æ‰¹æ¬¡ [1-100/100]
2025-12-29 18:45:53,523 - __main__ - INFO -   â†’ ç”ŸæˆBaselineç­”æ¡ˆ (100 æ¡)...
2025-12-29 18:45:53,523 - __main__ - INFO - æ‰¹é‡ç”ŸæˆBaselineç­”æ¡ˆ: 100 æ¡
INFO 12-29 18:45:57 [__init__.py:239] Automatically detected platform cuda.
2025-12-29 18:45:57,885 - inference.local_inference - INFO - ä½¿ç”¨ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES: 6,7
2025-12-29 18:45:57,886 - inference.local_inference - INFO - ============================================================
2025-12-29 18:45:57,886 - inference.local_inference - INFO - åˆå§‹åŒ–vLLMæœ¬åœ°æ¨¡å‹...
2025-12-29 18:45:57,886 - inference.local_inference - INFO - æ¨¡å‹è·¯å¾„: /home/share/hcz/qwen2.5-14b-awq
2025-12-29 18:45:57,886 - inference.local_inference - INFO - ============================================================
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 12-29 18:46:04 [config.py:585] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 12-29 18:46:04 [config.py:1519] Defaulting to use mp for distributed inference
INFO 12-29 18:46:04 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-29 18:46:09 [__init__.py:239] Automatically detected platform cuda.
INFO 12-29 18:46:11 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/models/qwen_dpo2_lora', speculative_config=None, tokenizer='/home/models/qwen_dpo2_lora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/models/qwen_dpo2_lora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 12-29 18:46:11 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-29 18:46:11 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_76d798d1'), local_subscribe_addr='ipc:///tmp/024c816b-7877-436a-9652-c0abaf9ec8d8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-29 18:46:15 [__init__.py:239] Automatically detected platform cuda.
WARNING 12-29 18:46:17 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f95c4daf6d0>
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:17 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d8718d3c'), local_subscribe_addr='ipc:///tmp/82bcb870-4d94-439b-8870-5cf23bbfda01', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-29 18:46:20 [__init__.py:239] Automatically detected platform cuda.
WARNING 12-29 18:46:23 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faeb8b7e550>
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:23 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4d9d46ea'), local_subscribe_addr='ipc:///tmp/fe0ac57e-3661-4f4d-bfef-0c2268b9fef4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:23 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:23 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:23 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:23 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:24 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_6,7.json
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:24 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_6,7.json
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:24 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_19c24f4b'), local_subscribe_addr='ipc:///tmp/e1be0464-c0ab-4b4b-b13a-1a6e2cd35971', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:24 [parallel_state.py:954] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:24 [parallel_state.py:954] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:24 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:24 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:24 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo2_lora...
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:24 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo2_lora...
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m WARNING 12-29 18:46:25 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m WARNING 12-29 18:46:25 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:04,  1.22it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:03,  1.19it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.19it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:03<00:01,  1.26it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:04<00:00,  1.24it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.29it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.26it/s]
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m 
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:30 [loader.py:447] Loading weights took 4.85 seconds
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:30 [gpu_model_runner.py:1186] Model loading took 13.9281 GB and 5.659699 seconds
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:30 [loader.py:447] Loading weights took 5.12 seconds
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:30 [gpu_model_runner.py:1186] Model loading took 13.9281 GB and 5.934662 seconds
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:40 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/25a18384dd/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:40 [backends.py:425] Dynamo bytecode transform time: 10.35 s
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:40 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/25a18384dd/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:40 [backends.py:425] Dynamo bytecode transform time: 10.35 s
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:41 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:41 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:46:51 [monitor.py:33] torch.compile takes 10.35 s in total
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:46:51 [monitor.py:33] torch.compile takes 10.35 s in total
INFO 12-29 18:46:53 [kv_cache_utils.py:566] GPU KV cache size: 552,192 tokens
INFO 12-29 18:46:53 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 16.85x
INFO 12-29 18:46:53 [kv_cache_utils.py:566] GPU KV cache size: 552,192 tokens
INFO 12-29 18:46:53 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 16.85x
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:47:14 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:47:15 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=1465673)[0;0m INFO 12-29 18:47:15 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.72 GiB
[1;36m(VllmWorker rank=1 pid=1465694)[0;0m INFO 12-29 18:47:15 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.72 GiB
INFO 12-29 18:47:15 [core.py:151] init engine (profile, create kv cache, warmup model) took 44.60 seconds
2025-12-29 18:47:15,194 - inference.local_inference - INFO - vLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ
2025-12-29 18:49:17,709 - __main__ - INFO -   â†’ ç”Ÿæˆå·®å¼‚åˆ†æ (100 æ¡)...
2025-12-29 18:49:17,710 - __main__ - INFO - æ‰¹é‡ç”Ÿæˆå·®å¼‚åˆ†æ: 100 æ¡
2025-12-29 18:57:40,522 - __main__ - INFO -   â†’ ç”ŸæˆRejectedåŸåˆ™ (100 æ¡)...
2025-12-29 18:57:40,523 - __main__ - INFO - æ‰¹é‡ç”ŸæˆåŸåˆ™ï¼ˆå¼±æ¨¡å‹ï¼‰: 100 æ¡
2025-12-29 19:16:19,071 - __main__ - INFO - æ‰¹æ¬¡ [1-100] æœ¬åœ°æ¨ç†å®Œæˆ
2025-12-29 19:16:19,072 - __main__ - INFO - é˜¶æ®µ1å®Œæˆ: å…±ç”Ÿæˆ 100 æ¡æœ¬åœ°æ¨ç†ç»“æœ
2025-12-29 19:16:19,072 - __main__ - INFO - ä¿å­˜vLLMå¤„ç†ç»“æœåˆ°: /home/metanew2/output/vllm_cache.json
2025-12-29 19:16:19,129 - __main__ - INFO - vLLMå¤„ç†ç»“æœå·²å®‰å…¨ä¿å­˜
2025-12-29 19:16:19,129 - __main__ - INFO - ============================================================
2025-12-29 19:16:19,129 - __main__ - INFO - é˜¶æ®µ2/3: APIå¹¶å‘ç”ŸæˆChosenï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
2025-12-29 19:16:19,129 - __main__ - INFO - ============================================================
2025-12-29 19:16:19,129 - __main__ - INFO - APIåˆ†æ‰¹å¤„ç†: æ¯æ‰¹ 30 æ¡ï¼Œå…± 4 æ‰¹
2025-12-29 19:16:19,129 - __main__ - INFO - APIæ‰¹æ¬¡ [1-30/100] å¼€å§‹å¤„ç†...
2025-12-29 19:16:19,129 - module.execute_module - INFO - å¯åŠ¨ 30 ä¸ªå¹¶å‘çº¿ç¨‹è°ƒç”¨å¼ºæ¨¡å‹API
APIå¹¶å‘è°ƒç”¨:   0%|          | 0/30 [00:00<?, ?it/s]APIå¹¶å‘è°ƒç”¨:   3%|â–         | 1/30 [00:13<06:38, 13.73s/it]APIå¹¶å‘è°ƒç”¨:   7%|â–‹         | 2/30 [00:13<02:40,  5.75s/it]APIå¹¶å‘è°ƒç”¨:  10%|â–ˆ         | 3/30 [00:15<01:44,  3.87s/it]APIå¹¶å‘è°ƒç”¨:  13%|â–ˆâ–        | 4/30 [00:16<01:06,  2.54s/it]APIå¹¶å‘è°ƒç”¨:  17%|â–ˆâ–‹        | 5/30 [00:16<00:47,  1.91s/it]APIå¹¶å‘è°ƒç”¨:  20%|â–ˆâ–ˆ        | 6/30 [00:17<00:33,  1.38s/it]APIå¹¶å‘è°ƒç”¨:  23%|â–ˆâ–ˆâ–       | 7/30 [00:17<00:25,  1.13s/it]APIå¹¶å‘è°ƒç”¨:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:18<00:19,  1.16it/s]APIå¹¶å‘è°ƒç”¨:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:18<00:14,  1.45it/s]APIå¹¶å‘è°ƒç”¨:  33%|â–ˆâ–ˆâ–ˆâ–      | 10/30 [00:18<00:10,  1.89it/s]APIå¹¶å‘è°ƒç”¨:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:18<00:05,  3.14it/s]APIå¹¶å‘è°ƒç”¨:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/30 [00:18<00:04,  3.68it/s]APIå¹¶å‘è°ƒç”¨:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:18<00:03,  4.26it/s]APIå¹¶å‘è°ƒç”¨:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:19<00:03,  4.58it/s]APIå¹¶å‘è°ƒç”¨:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/30 [00:19<00:05,  2.60it/s]APIå¹¶å‘è°ƒç”¨:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/30 [00:20<00:03,  3.63it/s]APIå¹¶å‘è°ƒç”¨:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:20<00:02,  4.12it/s]APIå¹¶å‘è°ƒç”¨:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 22/30 [00:21<00:02,  3.89it/s]APIå¹¶å‘è°ƒç”¨:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:21<00:02,  2.65it/s]APIå¹¶å‘è°ƒç”¨:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:22<00:02,  2.26it/s]APIå¹¶å‘è°ƒç”¨:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:24<00:02,  1.83it/s]APIå¹¶å‘è°ƒç”¨:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:24<00:01,  1.58it/s]APIå¹¶å‘è°ƒç”¨:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 28/30 [00:47<00:11,  5.90s/it]APIå¹¶å‘è°ƒç”¨:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:51<00:05,  5.34s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:25<00:00, 13.06s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:25<00:00,  2.84s/it]
2025-12-29 19:17:44,314 - __main__ - INFO - APIæ‰¹æ¬¡ [1-30] å®Œæˆ
2025-12-29 19:17:44,315 - __main__ - INFO - APIæ‰¹æ¬¡ [31-60/100] å¼€å§‹å¤„ç†...
2025-12-29 19:17:44,315 - module.execute_module - INFO - å¯åŠ¨ 30 ä¸ªå¹¶å‘çº¿ç¨‹è°ƒç”¨å¼ºæ¨¡å‹API
APIå¹¶å‘è°ƒç”¨:   0%|          | 0/30 [00:00<?, ?it/s]APIå¹¶å‘è°ƒç”¨:   3%|â–         | 1/30 [00:12<05:55, 12.26s/it]APIå¹¶å‘è°ƒç”¨:   7%|â–‹         | 2/30 [00:13<02:42,  5.81s/it]APIå¹¶å‘è°ƒç”¨:  10%|â–ˆ         | 3/30 [00:14<01:32,  3.41s/it]APIå¹¶å‘è°ƒç”¨:  17%|â–ˆâ–‹        | 5/30 [00:14<00:41,  1.64s/it]APIå¹¶å‘è°ƒç”¨:  20%|â–ˆâ–ˆ        | 6/30 [00:14<00:29,  1.23s/it]APIå¹¶å‘è°ƒç”¨:  23%|â–ˆâ–ˆâ–       | 7/30 [00:16<00:29,  1.30s/it]APIå¹¶å‘è°ƒç”¨:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:16<00:23,  1.06s/it]APIå¹¶å‘è°ƒç”¨:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:17<00:18,  1.13it/s]APIå¹¶å‘è°ƒç”¨:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:19<00:17,  1.08it/s]APIå¹¶å‘è°ƒç”¨:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/30 [00:19<00:11,  1.47it/s]APIå¹¶å‘è°ƒç”¨:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:19<00:08,  1.81it/s]APIå¹¶å‘è°ƒç”¨:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:20<00:06,  2.15it/s]APIå¹¶å‘è°ƒç”¨:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/30 [00:20<00:06,  2.22it/s]APIå¹¶å‘è°ƒç”¨:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:21<00:08,  1.60it/s]APIå¹¶å‘è°ƒç”¨:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:21<00:05,  2.06it/s]APIå¹¶å‘è°ƒç”¨:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:22<00:03,  2.57it/s]APIå¹¶å‘è°ƒç”¨:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 22/30 [00:25<00:07,  1.08it/s]APIå¹¶å‘è°ƒç”¨:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:26<00:05,  1.20it/s]APIå¹¶å‘è°ƒç”¨:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:26<00:04,  1.44it/s]APIå¹¶å‘è°ƒç”¨:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 25/30 [00:27<00:03,  1.51it/s]APIå¹¶å‘è°ƒç”¨:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:27<00:02,  1.87it/s]APIå¹¶å‘è°ƒç”¨:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:28<00:01,  1.79it/s]APIå¹¶å‘è°ƒç”¨:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 28/30 [00:29<00:01,  1.34it/s]APIå¹¶å‘è°ƒç”¨:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:47<00:05,  5.71s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:57<00:00,  6.99s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:57<00:00,  1.91s/it]
2025-12-29 19:18:41,788 - __main__ - INFO - APIæ‰¹æ¬¡ [31-60] å®Œæˆ
2025-12-29 19:18:41,789 - __main__ - INFO - APIæ‰¹æ¬¡ [61-90/100] å¼€å§‹å¤„ç†...
2025-12-29 19:18:41,789 - module.execute_module - INFO - å¯åŠ¨ 30 ä¸ªå¹¶å‘çº¿ç¨‹è°ƒç”¨å¼ºæ¨¡å‹API
APIå¹¶å‘è°ƒç”¨:   0%|          | 0/30 [00:00<?, ?it/s]APIå¹¶å‘è°ƒç”¨:   3%|â–         | 1/30 [00:07<03:38,  7.54s/it]APIå¹¶å‘è°ƒç”¨:   7%|â–‹         | 2/30 [00:11<02:39,  5.69s/it]APIå¹¶å‘è°ƒç”¨:  10%|â–ˆ         | 3/30 [00:13<01:45,  3.90s/it]APIå¹¶å‘è°ƒç”¨:  13%|â–ˆâ–        | 4/30 [00:13<01:02,  2.41s/it]APIå¹¶å‘è°ƒç”¨:  17%|â–ˆâ–‹        | 5/30 [00:14<00:41,  1.65s/it]APIå¹¶å‘è°ƒç”¨:  23%|â–ˆâ–ˆâ–       | 7/30 [00:14<00:19,  1.15it/s]APIå¹¶å‘è°ƒç”¨:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:15<00:21,  1.00it/s]APIå¹¶å‘è°ƒç”¨:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:16<00:18,  1.11it/s]APIå¹¶å‘è°ƒç”¨:  33%|â–ˆâ–ˆâ–ˆâ–      | 10/30 [00:16<00:15,  1.29it/s]APIå¹¶å‘è°ƒç”¨:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:17<00:14,  1.29it/s]APIå¹¶å‘è°ƒç”¨:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:17<00:10,  1.67it/s]APIå¹¶å‘è°ƒç”¨:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/30 [00:19<00:14,  1.15it/s]APIå¹¶å‘è°ƒç”¨:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:19<00:11,  1.35it/s]APIå¹¶å‘è°ƒç”¨:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:20<00:11,  1.29it/s]APIå¹¶å‘è°ƒç”¨:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/30 [00:20<00:08,  1.70it/s]APIå¹¶å‘è°ƒç”¨:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:21<00:06,  1.90it/s]APIå¹¶å‘è°ƒç”¨:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:21<00:05,  2.28it/s]APIå¹¶å‘è°ƒç”¨:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/30 [00:21<00:03,  2.89it/s]APIå¹¶å‘è°ƒç”¨:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:22<00:04,  2.03it/s]APIå¹¶å‘è°ƒç”¨:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 22/30 [00:24<00:05,  1.46it/s]APIå¹¶å‘è°ƒç”¨:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:24<00:03,  1.82it/s]APIå¹¶å‘è°ƒç”¨:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:25<00:05,  1.17it/s]APIå¹¶å‘è°ƒç”¨:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 25/30 [00:26<00:03,  1.41it/s]APIå¹¶å‘è°ƒç”¨:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:27<00:03,  1.07it/s]APIå¹¶å‘è°ƒç”¨:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:30<00:04,  1.40s/it]APIå¹¶å‘è°ƒç”¨:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 28/30 [00:30<00:02,  1.20s/it]APIå¹¶å‘è°ƒç”¨:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:31<00:00,  1.04it/s]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:56<00:00,  8.06s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:56<00:00,  1.88s/it]
2025-12-29 19:19:38,192 - __main__ - INFO - APIæ‰¹æ¬¡ [61-90] å®Œæˆ
2025-12-29 19:19:38,193 - __main__ - INFO - APIæ‰¹æ¬¡ [91-100/100] å¼€å§‹å¤„ç†...
2025-12-29 19:19:38,193 - module.execute_module - INFO - å¯åŠ¨ 30 ä¸ªå¹¶å‘çº¿ç¨‹è°ƒç”¨å¼ºæ¨¡å‹API
APIå¹¶å‘è°ƒç”¨:   0%|          | 0/10 [00:00<?, ?it/s]APIå¹¶å‘è°ƒç”¨:  10%|â–ˆ         | 1/10 [00:12<01:48, 12.04s/it]APIå¹¶å‘è°ƒç”¨:  20%|â–ˆâ–ˆ        | 2/10 [00:14<00:53,  6.63s/it]APIå¹¶å‘è°ƒç”¨:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:17,  2.99s/it]APIå¹¶å‘è°ƒç”¨:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:16<00:10,  2.17s/it]APIå¹¶å‘è°ƒç”¨:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:08,  2.12s/it]APIå¹¶å‘è°ƒç”¨:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:22<00:04,  2.01s/it]APIå¹¶å‘è°ƒç”¨:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:01,  1.54s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  6.58s/it]APIå¹¶å‘è°ƒç”¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.35s/it]
2025-12-29 19:20:21,685 - __main__ - INFO - APIæ‰¹æ¬¡ [91-100] å®Œæˆ
2025-12-29 19:20:21,685 - __main__ - INFO - é˜¶æ®µ2å®Œæˆ: å…±ç”Ÿæˆ 100 æ¡Chosenç»“æœ
2025-12-29 19:20:21,685 - __main__ - INFO - å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...
2025-12-29 19:20:21,686 - __main__ - INFO - âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡: 100 æ¡chosenå…¨éƒ¨éç©º
2025-12-29 19:20:21,686 - __main__ - INFO - ============================================================
2025-12-29 19:20:21,686 - __main__ - INFO - é˜¶æ®µ3/3: ç»„è£…DPOæ•°æ®å¹¶ä¿å­˜ä¸ºJSONLæ ¼å¼
2025-12-29 19:20:21,686 - __main__ - INFO - ============================================================
2025-12-29 19:20:21,686 - __main__ - INFO - é¢„æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...
2025-12-29 19:20:21,686 - __main__ - INFO - Chosenéç©ºç‡: 100/100 (100.0%)
2025-12-29 19:20:21,687 - __main__ - INFO - Rejectedéç©ºç‡: 100/100 (100.0%)
2025-12-29 19:20:21,687 - __main__ - INFO - âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡
ç»„è£…å¹¶ä¿å­˜JSONL:   0%|          | 0/100 [00:00<?, ?it/s]2025-12-29 19:20:21,721 - __main__ - INFO - å·²ä¿å­˜ 50/100 æ¡åˆ°JSONL
2025-12-29 19:20:21,752 - __main__ - INFO - å·²ä¿å­˜ 100/100 æ¡åˆ°JSONL
ç»„è£…å¹¶ä¿å­˜JSONL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1551.38it/s]
2025-12-29 19:20:21,768 - __main__ - INFO - DPOæ•°æ®ç”Ÿæˆå®Œæˆ: output/mmlu/dpo_college_computer_science.jsonl
2025-12-29 19:20:21,769 - __main__ - INFO - å…±ä¿å­˜ 100 æ¡æ•°æ®åˆ°JSONLæ ¼å¼
2025-12-29 19:20:25,594 - inference.local_inference - INFO - æ¸…ç†vLLMæ¨¡å‹...
2025-12-29 19:20:25,630 - inference.local_inference - INFO - CUDAç¼“å­˜å·²æ¸…ç†
âœ… å®Œæˆ: college_computer_science (100 æ¡, 7.8M)

----------------------------------------
å¤„ç†ä»»åŠ¡ [2/10]: college_mathematics
----------------------------------------
å¼€å§‹ç”Ÿæˆ DPO æ•°æ®...
è¾“å‡ºæ–‡ä»¶: output/mmlu/dpo_college_mathematics.jsonl
2025-12-29 19:20:26,708 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,708 - __main__ - INFO - æ•°æ®é›†åç§°: mmlu
2025-12-29 19:20:26,708 - __main__ - INFO - æ•°æ®é›†è·¯å¾„: dataset/mmlu/college_mathematics.json
2025-12-29 19:20:26,708 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,708 - __main__ - INFO - ä½¿ç”¨æ•°æ®é›†é€‚é…å±‚åŠ è½½: mmlu
2025-12-29 19:20:26,708 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,708 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] å¼€å§‹åŠ è½½æ•°æ®é›†: mmlu
2025-12-29 19:20:26,708 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] æ–‡ä»¶è·¯å¾„: dataset/mmlu/college_mathematics.json
2025-12-29 19:20:26,708 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,709 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] å·²åŠ è½½ JSON æ–‡ä»¶
2025-12-29 19:20:26,709 - __main__ - INFO - é¢„å¤„ç† MMLU æ•°æ®é›†: 100 æ¡
2025-12-29 19:20:26,709 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] é¢„å¤„ç†å®Œæˆ: 100 æ¡æœ‰æ•ˆæ•°æ®
2025-12-29 19:20:26,709 - __main__ - INFO - [æ•°æ®é›†é€‚é…å±‚] æ•°æ®æ ¼å¼å·²ç»Ÿä¸€ä¸º: {'question': str, 'answer': str}
2025-12-29 19:20:26,709 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,709 - __main__ - INFO - æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± 100 æ¡æ•°æ®
2025-12-29 19:20:26,709 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,709 - __main__ - INFO - Step 1: å¼€å§‹ç”ŸæˆDPOæ•°æ®
2025-12-29 19:20:26,709 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,709 - __main__ - INFO - æœªå‘ç°å·²æœ‰è¿›åº¦ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†
2025-12-29 19:20:26,710 - __main__ - INFO - å…±éœ€å¤„ç† 100 æ¡æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: 64
2025-12-29 19:20:26,710 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,710 - __main__ - INFO - é˜¶æ®µ1/3: vLLMåˆ†æ‰¹æœ¬åœ°æ¨ç†ï¼ˆæ‰€æœ‰æ•°æ®ï¼‰
2025-12-29 19:20:26,710 - __main__ - INFO - ============================================================
2025-12-29 19:20:26,710 - __main__ - INFO - å¤„ç†æ‰¹æ¬¡ [1-100/100]
2025-12-29 19:20:26,710 - __main__ - INFO -   â†’ ç”ŸæˆBaselineç­”æ¡ˆ (100 æ¡)...
2025-12-29 19:20:26,710 - __main__ - INFO - æ‰¹é‡ç”ŸæˆBaselineç­”æ¡ˆ: 100 æ¡
INFO 12-29 19:20:30 [__init__.py:239] Automatically detected platform cuda.
2025-12-29 19:20:31,265 - inference.local_inference - INFO - ä½¿ç”¨ç¯å¢ƒå˜é‡ CUDA_VISIBLE_DEVICES: 6,7
2025-12-29 19:20:31,266 - inference.local_inference - INFO - ============================================================
2025-12-29 19:20:31,266 - inference.local_inference - INFO - åˆå§‹åŒ–vLLMæœ¬åœ°æ¨¡å‹...
2025-12-29 19:20:31,266 - inference.local_inference - INFO - æ¨¡å‹è·¯å¾„: /home/share/hcz/qwen2.5-14b-awq
2025-12-29 19:20:31,266 - inference.local_inference - INFO - ============================================================
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 12-29 19:20:37 [config.py:585] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.
INFO 12-29 19:20:37 [config.py:1519] Defaulting to use mp for distributed inference
INFO 12-29 19:20:37 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-29 19:20:43 [__init__.py:239] Automatically detected platform cuda.
INFO 12-29 19:20:44 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/models/qwen_dpo2_lora', speculative_config=None, tokenizer='/home/models/qwen_dpo2_lora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/models/qwen_dpo2_lora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 12-29 19:20:44 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-29 19:20:44 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_782f3139'), local_subscribe_addr='ipc:///tmp/203ff87e-a504-41b3-843e-3df6c32c1024', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-29 19:20:48 [__init__.py:239] Automatically detected platform cuda.
WARNING 12-29 19:20:50 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2eb7eb3a50>
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:50 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_909eed0e'), local_subscribe_addr='ipc:///tmp/b40eded8-f535-451c-a711-2b3825652172', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-29 19:20:54 [__init__.py:239] Automatically detected platform cuda.
WARNING 12-29 19:20:56 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4265b376d0>
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:56 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_245e8c59'), local_subscribe_addr='ipc:///tmp/08d1ee33-cfab-4fd2-af7b-3d8c0318820a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:57 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:57 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:57 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:57 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:58 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_6,7.json
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:58 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_6,7.json
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:58 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2e56b954'), local_subscribe_addr='ipc:///tmp/4179340f-7347-4760-be82-0806ee0c812c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:58 [parallel_state.py:954] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:58 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:58 [parallel_state.py:954] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:58 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:20:58 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo2_lora...
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:20:58 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo2_lora...
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m WARNING 12-29 19:20:58 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m WARNING 12-29 19:20:58 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:04,  1.23it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:03,  1.17it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.17it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:03<00:01,  1.23it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:04<00:00,  1.21it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.29it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:04<00:00,  1.24it/s]
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m 
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:03 [loader.py:447] Loading weights took 4.92 seconds
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:03 [loader.py:447] Loading weights took 4.94 seconds
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:03 [gpu_model_runner.py:1186] Model loading took 13.9281 GB and 5.315257 seconds
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:03 [gpu_model_runner.py:1186] Model loading took 13.9281 GB and 5.341698 seconds
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:13 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/25a18384dd/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:13 [backends.py:425] Dynamo bytecode transform time: 9.92 s
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:14 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/25a18384dd/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:14 [backends.py:425] Dynamo bytecode transform time: 10.24 s
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:24 [monitor.py:33] torch.compile takes 9.92 s in total
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:24 [monitor.py:33] torch.compile takes 10.24 s in total
INFO 12-29 19:21:26 [kv_cache_utils.py:566] GPU KV cache size: 552,192 tokens
INFO 12-29 19:21:26 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 16.85x
INFO 12-29 19:21:26 [kv_cache_utils.py:566] GPU KV cache size: 552,192 tokens
INFO 12-29 19:21:26 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 16.85x
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:48 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:54 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=1466490)[0;0m INFO 12-29 19:21:54 [gpu_model_runner.py:1534] Graph capturing finished in 28 secs, took 0.72 GiB
[1;36m(VllmWorker rank=0 pid=1466473)[0;0m INFO 12-29 19:21:54 [gpu_model_runner.py:1534] Graph capturing finished in 28 secs, took 0.72 GiB
INFO 12-29 19:21:54 [core.py:151] init engine (profile, create kv cache, warmup model) took 50.82 seconds
2025-12-29 19:21:54,706 - inference.local_inference - INFO - vLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ
2025-12-29 19:23:08,362 - __main__ - INFO -   â†’ ç”Ÿæˆå·®å¼‚åˆ†æ (100 æ¡)...
2025-12-29 19:23:08,362 - __main__ - INFO - æ‰¹é‡ç”Ÿæˆå·®å¼‚åˆ†æ: 100 æ¡
2025-12-29 19:31:55,118 - __main__ - INFO -   â†’ ç”ŸæˆRejectedåŸåˆ™ (100 æ¡)...
2025-12-29 19:31:55,119 - __main__ - INFO - æ‰¹é‡ç”ŸæˆåŸåˆ™ï¼ˆå¼±æ¨¡å‹ï¼‰: 100 æ¡
