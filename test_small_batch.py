"""
å°æ‰¹æ¬¡æµ‹è¯•è„šæœ¬

ä»æ¯ä¸ªæ•°æ®é›†ä¸­æå–å‰128æ¡æ•°æ®è¿›è¡Œæµ‹è¯•
ç”¨äºå¿«é€ŸéªŒè¯å®Œæ•´çš„ DPO ç”Ÿæˆæµç¨‹
"""

import json
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from stage_first import load_and_preprocess_dataset, prepare_stage1
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æµ‹è¯•æ‰¹æ¬¡å¤§å°
TEST_BATCH_SIZE = 128

# å®šä¹‰è¦æµ‹è¯•çš„æ•°æ®é›†
TEST_DATASETS = [
    {
        "name": "bbh",
        "path": "dataset/bbh/boolean_expressions.json",
        "description": "BBH Boolean Expressions (æ¨ç†ä»»åŠ¡)"
    },
    {
        "name": "svamp",
        "path": "dataset/svamp/test.json",
        "description": "SVAMP (æ•°å­¦åº”ç”¨é¢˜)"
    },
    {
        "name": "gsm8k",
        "path": "dataset/gsm8k/test.jsonl",
        "description": "GSM8K (æ•°å­¦åº”ç”¨é¢˜)"
    },
    {
        "name": "math",
        "path": "dataset/math/test.jsonl",
        "description": "MATH (é«˜ç­‰æ•°å­¦é¢˜)"
    },
]


def create_small_test_dataset(dataset_name: str, dataset_path: str, output_path: str):
    """
    åˆ›å»ºå°æµ‹è¯•æ•°æ®é›†ï¼ˆå‰128æ¡ï¼‰
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        dataset_path: åŸå§‹æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
    """
    logger.info("=" * 80)
    logger.info(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
    logger.info(f"åŸå§‹è·¯å¾„: {dataset_path}")
    logger.info("=" * 80)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(dataset_path).exists():
        logger.warning(f"âš ï¸  æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {dataset_path}")
        return None
    
    try:
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        full_dataset = load_and_preprocess_dataset(dataset_name, dataset_path)
        logger.info(f"âœ… åŸå§‹æ•°æ®é‡: {len(full_dataset)} æ¡")
        
        # å–å‰128æ¡
        small_dataset = full_dataset[:TEST_BATCH_SIZE]
        logger.info(f"âœ… æµ‹è¯•æ•°æ®é‡: {len(small_dataset)} æ¡")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(small_dataset, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å·²ä¿å­˜åˆ°: {output_path}")
        logger.info(f"ğŸ“Š æ ·æœ¬é¢„è§ˆ:")
        logger.info(f"   Question: {small_dataset[0]['question'][:100]}...")
        logger.info(f"   Answer: {small_dataset[0]['answer'][:100]}...")
        
        return small_dataset
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return None


def run_test_for_dataset(dataset_name: str, test_data_path: str, description: str):
    """
    å¯¹å•ä¸ªæ•°æ®é›†è¿è¡Œæµ‹è¯•
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        description: æ•°æ®é›†æè¿°
    """
    logger.info("\n" + "=" * 80)
    logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•: {description}")
    logger.info("=" * 80)
    
    try:
        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(test_data_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®é‡: {len(dataset)} æ¡")
        
        # è¿è¡Œå®Œæ•´çš„ DPO ç”Ÿæˆæµç¨‹
        prepare_stage1(dataset)
        
        logger.info(f"âœ… {dataset_name} æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ {dataset_name} æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("ğŸ§ª å°æ‰¹æ¬¡æµ‹è¯•è„šæœ¬")
    print("=" * 80)
    print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {TEST_BATCH_SIZE} æ¡")
    print(f"æµ‹è¯•æ•°æ®é›†æ•°é‡: {len(TEST_DATASETS)}")
    print("\nè¯´æ˜:")
    print("  1. ä»æ¯ä¸ªæ•°æ®é›†æå–å‰128æ¡æ•°æ®")
    print("  2. ä¿å­˜ä¸ºæµ‹è¯•æ•°æ®æ–‡ä»¶")
    print("  3. è¿è¡Œå®Œæ•´çš„ DPO ç”Ÿæˆæµç¨‹")
    print("  4. éªŒè¯ç«¯åˆ°ç«¯åŠŸèƒ½")
    print("\n" + "=" * 80)
    
    # æ­¥éª¤1: åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    print("\nğŸ“¦ æ­¥éª¤1: åˆ›å»ºæµ‹è¯•æ•°æ®é›†\n")
    
    test_data_dir = Path("data/test_small_batch")
    test_datasets_info = []
    
    for ds in TEST_DATASETS:
        output_path = test_data_dir / f"{ds['name']}_test_{TEST_BATCH_SIZE}.json"
        
        result = create_small_test_dataset(
            dataset_name=ds['name'],
            dataset_path=ds['path'],
            output_path=str(output_path)
        )
        
        if result:
            test_datasets_info.append({
                "name": ds['name'],
                "path": str(output_path),
                "description": ds['description'],
                "count": len(result)
            })
    
    if not test_datasets_info:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ›å»ºä»»ä½•æµ‹è¯•æ•°æ®é›†")
        return
    
    print("\n" + "=" * 80)
    print(f"âœ… æˆåŠŸåˆ›å»º {len(test_datasets_info)} ä¸ªæµ‹è¯•æ•°æ®é›†")
    print("=" * 80)
    
    # æ­¥éª¤2: é€‰æ‹©è¦æµ‹è¯•çš„æ•°æ®é›†
    print("\nğŸ“‹ å¯ç”¨çš„æµ‹è¯•æ•°æ®é›†:")
    for i, ds in enumerate(test_datasets_info, 1):
        print(f"  {i}. {ds['name']:10s} - {ds['description']} ({ds['count']} æ¡)")
    
    print("\n" + "=" * 80)
    print("è¯·é€‰æ‹©è¦æµ‹è¯•çš„æ•°æ®é›†:")
    print("  è¾“å…¥æ•°å­— (1-{})ï¼Œæˆ– 'all' æµ‹è¯•å…¨éƒ¨ï¼Œæˆ– 'q' é€€å‡º".format(len(test_datasets_info)))
    print("=" * 80)
    
    choice = input("\nè¯·é€‰æ‹©: ").strip().lower()
    
    if choice == 'q':
        print("\nğŸ‘‹ å·²é€€å‡º")
        return
    
    # æ­¥éª¤3: è¿è¡Œæµ‹è¯•
    print("\n" + "=" * 80)
    print("ğŸš€ æ­¥éª¤2: è¿è¡Œ DPO ç”Ÿæˆæµ‹è¯•")
    print("=" * 80)
    
    results = {}
    
    if choice == 'all':
        # æµ‹è¯•æ‰€æœ‰æ•°æ®é›†
        for ds in test_datasets_info:
            success = run_test_for_dataset(
                dataset_name=ds['name'],
                test_data_path=ds['path'],
                description=ds['description']
            )
            results[ds['name']] = success
    else:
        # æµ‹è¯•å•ä¸ªæ•°æ®é›†
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(test_datasets_info):
                ds = test_datasets_info[idx]
                success = run_test_for_dataset(
                    dataset_name=ds['name'],
                    test_data_path=ds['path'],
                    description=ds['description']
                )
                results[ds['name']] = success
            else:
                print(f"\nâŒ æ— æ•ˆçš„é€‰æ‹©: {choice}")
                return
        except ValueError:
            print(f"\nâŒ æ— æ•ˆçš„è¾“å…¥: {choice}")
            return
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    for name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {name:15s}: {status}")
    
    passed = sum(1 for s in results.values() if s)
    total = len(results)
    
    print("\n" + "-" * 80)
    print(f"  æ€»è®¡: {passed}/{total} é€šè¿‡")
    print("=" * 80)
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    output_file = Path("output/dpo_final.jsonl")
    if output_file.exists():
        print(f"\nâœ… è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.2f} KB")
        
        # æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®
        print("\nğŸ“‹ è¾“å‡ºæ ·æœ¬ (ç¬¬1æ¡):")
        with open(output_file, 'r', encoding='utf-8') as f:
            first_line = f.readline()
            sample = json.loads(first_line)
            print(json.dumps(sample, indent=2, ensure_ascii=False)[:500] + "...")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!\n")


if __name__ == "__main__":
    main()
