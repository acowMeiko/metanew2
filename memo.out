nohup: ignoring input
2025-12-30 21:38:36,615 - __main__ - INFO - ============================================================
2025-12-30 21:38:36,615 - __main__ - INFO - Step 2: ÂºÄÂßãÊõ¥Êñ∞Memory
2025-12-30 21:38:36,615 - __main__ - INFO - ============================================================
2025-12-30 21:38:36,616 - __main__ - INFO - MemoryManager ÂàùÂßãÂåñÂÆåÊàê
2025-12-30 21:38:36,624 - __main__ - INFO - Èò∂ÊÆµ1/3: Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂíåËøáÊª§
2025-12-30 21:38:36,636 - __main__ - INFO - ÂáÜÂ§áÊâπÂ§ÑÁêÜ 398 Êù°Êï∞ÊçÆ
2025-12-30 21:38:36,637 - __main__ - INFO - Èò∂ÊÆµ2/3: ÊâπÈáèÁîüÊàê‰ªªÂä°ÊèèËø∞ÂíåÂéüÂàô
2025-12-30 21:38:36,637 - __main__ - INFO - ÊâπÈáèÁîüÊàê‰ªªÂä°ÊèèËø∞...
INFO 12-30 21:38:40 [__init__.py:239] Automatically detected platform cuda.
2025-12-30 21:38:41,036 - inference.local_inference - INFO - ‰ΩøÁî®ÁéØÂ¢ÉÂèòÈáè CUDA_VISIBLE_DEVICES: 0,1,2,3
2025-12-30 21:38:41,036 - inference.local_inference - INFO - ============================================================
2025-12-30 21:38:41,037 - inference.local_inference - INFO - ÂàùÂßãÂåñvLLMÊú¨Âú∞Ê®°Âûã...
2025-12-30 21:38:41,037 - inference.local_inference - INFO - Ê®°ÂûãË∑ØÂæÑ: /home/share/hcz/qwen2.5-14b-awq
2025-12-30 21:38:41,037 - inference.local_inference - INFO - ============================================================
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 12-30 21:38:46 [config.py:585] This model supports multiple tasks: {'reward', 'classify', 'embed', 'score', 'generate'}. Defaulting to 'generate'.
INFO 12-30 21:38:47 [config.py:1519] Defaulting to use mp for distributed inference
INFO 12-30 21:38:47 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-30 21:38:48 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/models/qwen_dpo3_lora', speculative_config=None, tokenizer='/home/models/qwen_dpo3_lora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/models/qwen_dpo3_lora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 12-30 21:38:48 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-30 21:38:48 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_0e78b38c'), local_subscribe_addr='ipc:///tmp/413fe400-dc2f-4359-978b-897986b13be4', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 12-30 21:38:48 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2d403570d0>
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:48 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d06c914e'), local_subscribe_addr='ipc:///tmp/eac917e7-6cbd-4c23-8e84-1c601fff9e7d', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 12-30 21:38:49 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2afeed6e50>
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:49 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cf68858a'), local_subscribe_addr='ipc:///tmp/45014b43-a9a0-4fb7-99f4-c9c30a23e086', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 12-30 21:38:49 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2afeed6e50>
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:49 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6e3434e1'), local_subscribe_addr='ipc:///tmp/c493d458-08ca-485b-80c8-5db4e6bf89ec', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 12-30 21:38:50 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f2afeed6e50>
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:50 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dabc6706'), local_subscribe_addr='ipc:///tmp/d4f74f17-053b-4273-afec-ad5b78e1264e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:50 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:50 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:50 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:50 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:50 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:50 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:50 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:50 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:51 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:51 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8a90a8c6'), local_subscribe_addr='ipc:///tmp/699a1793-8dac-4f65-93c9-42467f430f2a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:51 [parallel_state.py:954] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:51 [parallel_state.py:954] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:51 [parallel_state.py:954] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:51 [parallel_state.py:954] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:51 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:51 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:51 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:51 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:51 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo3_lora...
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:51 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo3_lora...
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:51 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo3_lora...
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:51 [gpu_model_runner.py:1174] Starting to load model /home/models/qwen_dpo3_lora...
[1;36m(VllmWorker rank=1 pid=70531)[0;0m WARNING 12-30 21:38:52 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=70519)[0;0m WARNING 12-30 21:38:52 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=70558)[0;0m WARNING 12-30 21:38:52 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=2 pid=70544)[0;0m WARNING 12-30 21:38:52 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:01,  2.71it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:00<00:01,  2.30it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:01<00:01,  2.22it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:01<00:00,  2.21it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:02<00:00,  2.22it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:02<00:00,  2.38it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:02<00:00,  2.32it/s]
[1;36m(VllmWorker rank=0 pid=70519)[0;0m 
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:54 [loader.py:447] Loading weights took 2.61 seconds
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:54 [loader.py:447] Loading weights took 2.62 seconds
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:54 [loader.py:447] Loading weights took 2.61 seconds
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:54 [loader.py:447] Loading weights took 2.61 seconds
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:38:54 [gpu_model_runner.py:1186] Model loading took 6.9460 GB and 2.958460 seconds
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:38:54 [gpu_model_runner.py:1186] Model loading took 6.9460 GB and 2.958061 seconds
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:38:54 [gpu_model_runner.py:1186] Model loading took 6.9460 GB and 2.960784 seconds
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:38:55 [gpu_model_runner.py:1186] Model loading took 6.9460 GB and 2.959780 seconds
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:05 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/42b8e6bd16/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:05 [backends.py:425] Dynamo bytecode transform time: 10.11 s
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:05 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/42b8e6bd16/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:05 [backends.py:425] Dynamo bytecode transform time: 10.19 s
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:05 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/42b8e6bd16/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:05 [backends.py:425] Dynamo bytecode transform time: 10.22 s
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:05 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/42b8e6bd16/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:05 [backends.py:425] Dynamo bytecode transform time: 10.23 s
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:05 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:05 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:05 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:05 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:14 [monitor.py:33] torch.compile takes 10.19 s in total
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:14 [monitor.py:33] torch.compile takes 10.11 s in total
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:14 [monitor.py:33] torch.compile takes 10.23 s in total
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:14 [monitor.py:33] torch.compile takes 10.22 s in total
INFO 12-30 21:39:16 [kv_cache_utils.py:566] GPU KV cache size: 1,251,264 tokens
INFO 12-30 21:39:16 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 38.19x
INFO 12-30 21:39:16 [kv_cache_utils.py:566] GPU KV cache size: 1,247,168 tokens
INFO 12-30 21:39:16 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 38.06x
INFO 12-30 21:39:16 [kv_cache_utils.py:566] GPU KV cache size: 1,247,168 tokens
INFO 12-30 21:39:16 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 38.06x
INFO 12-30 21:39:16 [kv_cache_utils.py:566] GPU KV cache size: 1,251,264 tokens
INFO 12-30 21:39:16 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 38.19x
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:37 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:37 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:37 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:38 [custom_all_reduce.py:229] Registering 6499 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=70558)[0;0m INFO 12-30 21:39:38 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.71 GiB
[1;36m(VllmWorker rank=2 pid=70544)[0;0m INFO 12-30 21:39:38 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.71 GiB
[1;36m(VllmWorker rank=1 pid=70531)[0;0m INFO 12-30 21:39:38 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.71 GiB
[1;36m(VllmWorker rank=0 pid=70519)[0;0m INFO 12-30 21:39:38 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 0.71 GiB
INFO 12-30 21:39:38 [core.py:151] init engine (profile, create kv cache, warmup model) took 43.54 seconds
2025-12-30 21:39:38,603 - inference.local_inference - INFO - vLLMÊ®°ÂûãÂàùÂßãÂåñÂÆåÊàê
2025-12-30 21:40:36,787 - __main__ - INFO - ÊèêÂèñ chosen ÂéüÂàô...
2025-12-30 21:40:36,793 - __main__ - INFO - ‰øùÂ≠ò‰∏≠Èó¥ÁîüÊàêÁªìÊûúÂà∞: /home/metanew2/output/stage2_generated.json
2025-12-30 21:40:36,812 - __main__ - INFO - Èò∂ÊÆµ3/3: Ëß£ÊûêÁªìÊûúÂπ∂Êõ¥Êñ∞Memory
Êõ¥Êñ∞Memory:   0%|          | 0/398 [00:00<?, ?it/s]Êõ¥Êñ∞Memory:   0%|          | 1/398 [00:17<1:56:47, 17.65s/it]2025-12-30 21:40:54,467 - __main__ - WARNING - Á¨¨ 1 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:   1%|          | 3/398 [00:37<1:17:26, 11.76s/it]Êõ¥Êñ∞Memory:   1%|          | 4/398 [00:52<1:24:57, 12.94s/it]Êõ¥Êñ∞Memory:   1%|‚ñè         | 5/398 [01:09<1:34:30, 14.43s/it]Êõ¥Êñ∞Memory:   2%|‚ñè         | 6/398 [01:28<1:44:09, 15.94s/it]Êõ¥Êñ∞Memory:   2%|‚ñè         | 7/398 [01:47<1:49:36, 16.82s/it]Êõ¥Êñ∞Memory:   2%|‚ñè         | 8/398 [02:39<3:00:14, 27.73s/it]2025-12-30 21:43:16,376 - __main__ - WARNING - Á¨¨ 8 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:   3%|‚ñé         | 10/398 [03:36<3:00:56, 27.98s/it]2025-12-30 21:44:12,899 - __main__ - WARNING - Á¨¨ 10 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:44:12,899 - __main__ - WARNING - Á¨¨ 11 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:44:12,899 - __main__ - WARNING - Á¨¨ 12 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:   4%|‚ñé         | 14/398 [03:53<1:32:50, 14.51s/it]Êõ¥Êñ∞Memory:   4%|‚ñç         | 15/398 [04:12<1:37:27, 15.27s/it]Êõ¥Êñ∞Memory:   4%|‚ñç         | 16/398 [04:32<1:42:00, 16.02s/it]Êõ¥Êñ∞Memory:   4%|‚ñç         | 17/398 [04:50<1:44:49, 16.51s/it]Êõ¥Êñ∞Memory:   5%|‚ñç         | 18/398 [05:07<1:45:36, 16.68s/it]Êõ¥Êñ∞Memory:   5%|‚ñç         | 19/398 [05:25<1:46:42, 16.89s/it]Êõ¥Êñ∞Memory:   5%|‚ñå         | 20/398 [06:05<2:26:02, 23.18s/it]Êõ¥Êñ∞Memory:   5%|‚ñå         | 21/398 [06:20<2:10:38, 20.79s/it]Êõ¥Êñ∞Memory:   6%|‚ñå         | 22/398 [06:37<2:04:43, 19.90s/it]Êõ¥Êñ∞Memory:   6%|‚ñå         | 23/398 [06:56<2:03:05, 19.69s/it]Êõ¥Êñ∞Memory:   6%|‚ñå         | 24/398 [07:14<1:59:40, 19.20s/it]Êõ¥Êñ∞Memory:   6%|‚ñã         | 25/398 [07:33<1:58:24, 19.05s/it]Êõ¥Êñ∞Memory:   7%|‚ñã         | 26/398 [07:51<1:56:39, 18.82s/it]Êõ¥Êñ∞Memory:   7%|‚ñã         | 27/398 [08:29<2:30:18, 24.31s/it]Êõ¥Êñ∞Memory:   7%|‚ñã         | 28/398 [08:48<2:19:48, 22.67s/it]Êõ¥Êñ∞Memory:   7%|‚ñã         | 29/398 [09:06<2:11:43, 21.42s/it]2025-12-30 21:49:43,288 - __main__ - WARNING - Á¨¨ 29 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:   8%|‚ñä         | 31/398 [09:24<1:35:36, 15.63s/it]2025-12-30 21:50:00,969 - __main__ - WARNING - Á¨¨ 31 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:50:00,969 - __main__ - WARNING - Á¨¨ 32 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:   9%|‚ñä         | 34/398 [09:42<1:06:16, 10.92s/it]Êõ¥Êñ∞Memory:   9%|‚ñâ         | 35/398 [10:02<1:16:33, 12.65s/it]2025-12-30 21:50:39,400 - __main__ - WARNING - Á¨¨ 35 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:50:39,400 - __main__ - WARNING - Á¨¨ 36 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  10%|‚ñâ         | 38/398 [10:20<57:29,  9.58s/it]  Êõ¥Êñ∞Memory:  10%|‚ñâ         | 39/398 [10:34<1:02:48, 10.50s/it]Êõ¥Êñ∞Memory:  10%|‚ñà         | 40/398 [10:54<1:14:12, 12.44s/it]2025-12-30 21:51:31,784 - __main__ - WARNING - Á¨¨ 40 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:51:31,785 - __main__ - WARNING - Á¨¨ 41 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  11%|‚ñà         | 43/398 [11:13<56:11,  9.50s/it]  Êõ¥Êñ∞Memory:  11%|‚ñà         | 44/398 [12:07<1:43:22, 17.52s/it]Êõ¥Êñ∞Memory:  11%|‚ñà‚ñè        | 45/398 [12:27<1:47:00, 18.19s/it]2025-12-30 21:53:04,718 - __main__ - WARNING - Á¨¨ 45 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  12%|‚ñà‚ñè        | 47/398 [12:46<1:27:10, 14.90s/it]Êõ¥Êñ∞Memory:  12%|‚ñà‚ñè        | 48/398 [13:05<1:32:10, 15.80s/it]2025-12-30 21:53:42,518 - __main__ - WARNING - Á¨¨ 48 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 21:53:42,518 - __main__ - WARNING - Á¨¨ 49 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Invalid \escape: line 1 column 58 (char 57)ÔºåË∑≥Ëøá
2025-12-30 21:53:42,518 - __main__ - WARNING - Á¨¨ 50 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  13%|‚ñà‚ñé        | 52/398 [13:27<58:21, 10.12s/it]  2025-12-30 21:54:03,970 - __main__ - WARNING - Á¨¨ 52 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  14%|‚ñà‚ñé        | 54/398 [14:19<1:23:43, 14.60s/it]Êõ¥Êñ∞Memory:  14%|‚ñà‚ñç        | 55/398 [14:39<1:29:06, 15.59s/it]Êõ¥Êñ∞Memory:  14%|‚ñà‚ñç        | 56/398 [14:58<1:32:11, 16.17s/it]2025-12-30 21:55:35,394 - __main__ - WARNING - Á¨¨ 56 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  15%|‚ñà‚ñç        | 58/398 [15:18<1:18:41, 13.89s/it]2025-12-30 21:55:54,838 - __main__ - WARNING - Á¨¨ 58 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)ÔºåË∑≥Ëøá
2025-12-30 21:55:54,838 - __main__ - WARNING - Á¨¨ 59 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Invalid \escape: line 1 column 63 (char 62)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  15%|‚ñà‚ñå        | 61/398 [15:57<1:16:29, 13.62s/it]Êõ¥Êñ∞Memory:  16%|‚ñà‚ñå        | 62/398 [16:18<1:23:01, 14.83s/it]Êõ¥Êñ∞Memory:  16%|‚ñà‚ñå        | 63/398 [16:36<1:26:16, 15.45s/it]Êõ¥Êñ∞Memory:  16%|‚ñà‚ñå        | 64/398 [16:53<1:27:19, 15.69s/it]Êõ¥Êñ∞Memory:  16%|‚ñà‚ñã        | 65/398 [17:13<1:33:27, 16.84s/it]Êõ¥Êñ∞Memory:  17%|‚ñà‚ñã        | 66/398 [17:35<1:40:26, 18.15s/it]2025-12-30 21:58:12,426 - __main__ - WARNING - Á¨¨ 66 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  17%|‚ñà‚ñã        | 68/398 [17:56<1:21:54, 14.89s/it]Êõ¥Êñ∞Memory:  17%|‚ñà‚ñã        | 69/398 [18:15<1:26:38, 15.80s/it]2025-12-30 21:58:52,042 - __main__ - WARNING - Á¨¨ 69 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  18%|‚ñà‚ñä        | 71/398 [18:34<1:12:22, 13.28s/it]2025-12-30 21:59:10,962 - __main__ - WARNING - Á¨¨ 71 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  18%|‚ñà‚ñä        | 73/398 [18:50<1:01:35, 11.37s/it]Êõ¥Êñ∞Memory:  19%|‚ñà‚ñä        | 74/398 [19:08<1:09:34, 12.88s/it]2025-12-30 21:59:45,702 - __main__ - WARNING - Á¨¨ 74 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  19%|‚ñà‚ñâ        | 76/398 [19:27<1:01:47, 11.51s/it]Êõ¥Êñ∞Memory:  19%|‚ñà‚ñâ        | 77/398 [19:47<1:10:50, 13.24s/it]2025-12-30 22:00:23,847 - __main__ - WARNING - Á¨¨ 77 È°πËß£Êûê‰ªªÂä°ÊèèËø∞Â§±Ë¥•: Invalid \escape: line 2 column 50 (char 51)ÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  20%|‚ñà‚ñâ        | 79/398 [20:06<1:03:33, 11.96s/it]Êõ¥Êñ∞Memory:  20%|‚ñà‚ñà        | 80/398 [20:29<1:15:30, 14.25s/it]Êõ¥Êñ∞Memory:  20%|‚ñà‚ñà        | 81/398 [20:48<1:20:32, 15.24s/it]Êõ¥Êñ∞Memory:  21%|‚ñà‚ñà        | 82/398 [21:06<1:24:49, 16.10s/it]Êõ¥Êñ∞Memory:  21%|‚ñà‚ñà        | 83/398 [21:27<1:30:53, 17.31s/it]Êõ¥Êñ∞Memory:  21%|‚ñà‚ñà        | 84/398 [21:52<1:41:11, 19.34s/it]2025-12-30 22:02:29,001 - __main__ - WARNING - Á¨¨ 84 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 22:02:29,002 - __main__ - WARNING - Á¨¨ 85 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
2025-12-30 22:02:29,002 - __main__ - WARNING - Á¨¨ 86 È°πÊó†Ê≥ïÊèêÂèñ‰ªªÂä°ÊèèËø∞JSONÔºåË∑≥Ëøá
Êõ¥Êñ∞Memory:  22%|‚ñà‚ñà‚ñè       | 88/398 [22:12<54:58, 10.64s/it]  Êõ¥Êñ∞Memory:  22%|‚ñà‚ñà‚ñè       | 89/398 [22:30<1:01:56, 12.03s/it]